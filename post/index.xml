<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 467 notes</title>
    <link>https://srn221B.github.io/post/</link>
    <description>Recent content in Posts on 467 notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 17 Jan 2021 20:54:56 +0900</lastBuildDate>
    
	<atom:link href="https://srn221B.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Dockerを使ったGrafanaのDatasourceとDashboardの起動時追加方法</title>
      <link>https://srn221B.github.io/content4/</link>
      <pubDate>Sun, 17 Jan 2021 20:54:56 +0900</pubDate>
      
      <guid>https://srn221B.github.io/content4/</guid>
      <description>こんにちは ブログ書くの久々すぎて書き方忘れた。うそです。
去年の卒研終わったあたりから作り始めたこのサイトですが、あまりにも初期状態すぎたので、重い腰をあげて「HugoのthemaをBeautifulHugoからCupperに変更」「repositoryを整理」の２点をしました。一新したのでブログ書くモチベもこれで上がるはず（？）仕事もだいぶ慣れてきたので2022年は些細なことでももう少し書いていきたいな。。。
タイトル通り 「Dockerを使ったGrafanaのDatasourceとDashboardの起動時追加方法」のメモを簡単に備忘録としてまとめておきます。
Datasourceの起動時追加方法について Dashboardの起動時追加を設定する前に、まずはこっちを作っていきます。
構成 . ├── docker-compose.yml ├── flask │ ├── Dockerfile │ └── app │ └── app.py ├── grafana │ ├── Dockerfile │ └── datasource.yml └── prometheus.yml  ここでは grafanaで可視化するサーバーはprometheusで、prometheusでの監視はflaskを用いた自作exporterとしています。
flask Dockerfile FROM ubuntu:latest RUN apt-get update RUN apt-get install python3 python3-pip -y RUN pip3 install flask prometheus-client RUN mkdir /app  app.py pythonのPrometheusClientライブラリを使ってexporter化しましした。testしやすいようにcurl http://localhost:3000/hogeでGauge型のmetricsが増減するexporterです。
from flask import Flask,render_template,request import json import queue from werkzeug.middleware.dispatcher import DispatcherMiddleware from prometheus_client import make_wsgi_app,Gauge app = Flask(__name__) G1 = Gauge(&#39;Gauge1&#39;,&#39;Gauge test&#39;) G2 = Gauge(&#39;Gauge2&#39;,&#39;Gauge test&#39;) @app.</description>
    </item>
    
    <item>
      <title>zeppelinからcassandraとsparkを使う</title>
      <link>https://srn221B.github.io/content3/</link>
      <pubDate>Sun, 20 Sep 2020 20:51:54 +0900</pubDate>
      
      <guid>https://srn221B.github.io/content3/</guid>
      <description>タイトル通り。zeppelinからcassandraとsparkを使うまで簡単に構築メモ。 kubernetesとhelm 3.3.1を使って構築する。sparkとzeppelinのインストール→Cassandraのインストールの流れで説明。
Pod単位で弄れるが為だけにkubernetesを使います。Namespace分けたりIngress追加したりなどIaCならではの機能を活かした実装はここではしません。悪しからず。
sparkとzeppelin  リポジトリはこれ デプロイすると、MasterPod1台、WorkerPod1台が動く。 Zepplin上のファイルはデフォルトで記憶される。  values.yamlにおいてServicePortの編集  デフォルトのままだとsparkWebUIとzeppelinのServicePortが競合する。なので、どちらかのServicePortを変更する。 ここではsparkWebUIのServicePortを8080から8081に変更する。  $ vim spark/values.yaml  ... WebUi: Name: webui ServicePort: 8081 ContainerPort: 8080 ...  インストール $ helm install spark ./spark  インストール確認  defaultのnamespaceにインストールされていることを確認。  $ kubectl get pod  spark-master-877d79587-shv7g 1/1 Running spark-worker-87d4579f4-7fsg4 1/1 Running spark-worker-87d4579f4-lb7ft 1/1 Running spark-worker-87d4579f4-tvcnb 1/1 Running spark-zeppelin-85d6c884d8-289mb 1/1 Running  zeppelinとSparkWebUIにアクセスの確認  ブラウザ上からlocalhost:8080とlocalhost:8081へのアクセスへの確認ができれば完了。
  cassandra  リポジトリはこれ user/passwordを設定しやすそうだったのでbitnamiを選択。  values.</description>
    </item>
    
    <item>
      <title>IntelliJ IDEAでのSparkアプリケーション開発方法（Scala編）</title>
      <link>https://srn221B.github.io/content2/</link>
      <pubDate>Sat, 04 Apr 2020 15:40:59 +0900</pubDate>
      
      <guid>https://srn221B.github.io/content2/</guid>
      <description>前回に引き続きSpark環境構築系のおはなし。
Scala、何で開発してますか
VScode，AtomなどのエディタやEclipse，NetBeansなどのIDE，開発内容によって様々だと思います。
自分は簡単なコードだとScala(Metals)とScalaSnippetsの拡張機能を入れてVscodeを利用，Pluginを利用するものだったり複雑なコードだとIntelliJ IDEAを利用しています。
SparkアプリケーションをIntelliJ IDEAで開発する方法がとても便利だったので記録に残しておきます。
前提条件  Spark2.4がインストールされていること IntelliJIDEAがインストールされていること IntelliJIDEAでScalaを開発することができること
（N予備校のScala基礎コースで開発環境構築方法およびScalaの基礎が大変わかりやすかったです） Java8がインストールされていること JDK8を用いて開発できること
（[File]=&amp;gt;[Project Structure&amp;hellip;]=&amp;gt;左カラムのSDKsを選択=&amp;gt;JDKhomepathを変更からJDKの追加はできます）  環境  OS：Mac Spark：2.4.5 IntelliJIDEA：2019.3  プロジェクト作成 Scala=&amp;gt;sbt でプロジェクトを作成します。
Name，Location：任意
JDK：1.8
sbt，Scala：現在安定して動くもの（ここではsbt1.2.8，Scala2.11.0）
入力が終わったら[Finish]を押します。
Sparkを使えるようにする targetの配下にあるbuild.sbtにSparkのPluginを追加します。バージョンなどはMVNREPOSITORYサイトを参考にします。
name := &amp;quot;sparktest&amp;quot; version := &amp;quot;0.1&amp;quot; scalaVersion := &amp;quot;2.11.0&amp;quot; libraryDependencies ++= Seq( &amp;quot;org.apache.spark&amp;quot; %% &amp;quot;spark-core&amp;quot; % &amp;quot;2.4.0&amp;quot;, &amp;quot;org.apache.spark&amp;quot; %% &amp;quot;spark-sql&amp;quot; % &amp;quot;2.4.0&amp;quot;, &amp;quot;org.apache.spark&amp;quot; %% &amp;quot;spark-mllib&amp;quot; % &amp;quot;2.4.3&amp;quot;, &amp;quot;org.apache.spark&amp;quot; %% &amp;quot;spark-graphx&amp;quot; % &amp;quot;2.4.0&amp;quot;, &amp;quot;org.apache.spark&amp;quot; %% &amp;quot;spark-streaming&amp;quot; % &amp;quot;2.4.0&amp;quot; )  無事インポートが成功したらSpark環境は完了です。</description>
    </item>
    
    <item>
      <title>pysparkをjupyter-notebookから使う</title>
      <link>https://srn221B.github.io/content1/</link>
      <pubDate>Sat, 28 Mar 2020 00:08:00 +0900</pubDate>
      
      <guid>https://srn221B.github.io/content1/</guid>
      <description>今までpysparkをいじるときはコンソールでネチネチやっていたのですが、 pythonをjupyter-notebookで開発しているときに便利だなあと思ったので、思い切ってpysparkをいじるときもjupyter-notebookを使うようにしました。その時の備忘録。
前提条件  Sparkがインストールされていること（この記事がわかりやすかったです。) jupyterがインストールされていること(pipまたはcondaでインストール)  環境  OS：Mac python：Python 3.7.6 Spark：2.4.5 jupyter-notebook：6.0.3  Sparkの設定 Spark上の環境ファイルのテンプレート(spark-env-sh.template)をコピーします。
$ cd $SPARK_HOME/conf $ cp spark-env.sh.template spark-env.sh  コピーしたファイルを書き換えていきます。
$ vim spark-env.sh export PYSPARK_PYTHON=/usr/local/bin/python3 #pythonの場所 export PYSPARK_DRIVER_PYTHON=jupyter export PYSPARK_DRIVER_PYTHON_OPTS=&amp;quot;notebook&amp;quot;  環境変数の設定 以下の環境変数を.bashrc,.bash_profileに書きます。
$ vim ~/.bashrc SPARK_HOME=#Sparkをインストールした場所 PATH=$SPARK_HOME/bin:$PATH  動作確認 早速、jupyterを使って簡単な演算を実装して見ます。
$ pyspark http://localhost:8888/?token= *Token*が表示されます  http://localhost:8888/loginを開きます。表示されたTokenを元にログインします。
Pythonファイルを作ります（New&amp;gt;python3）
以下のようにsc(SparkSession)が使えていたら完了。
終わり 快適な開発環境が整いました:heart_eyes:</description>
    </item>
    
  </channel>
</rss>