<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 467memo</title>
    <link>https://srn221B.github.io/post/</link>
    <description>Recent content in Posts on 467memo</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 04 Apr 2020 15:40:59 +0900</lastBuildDate>
    
	<atom:link href="https://srn221B.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>IntelliJ IDEAでのSparkアプリケーション開発方法（Scala編）</title>
      <link>https://srn221B.github.io/post/content2/</link>
      <pubDate>Sat, 04 Apr 2020 15:40:59 +0900</pubDate>
      
      <guid>https://srn221B.github.io/post/content2/</guid>
      <description>前回に引き続きSpark環境構築系のおはなし。
Scala、何で開発してますか
VScode，AtomなどのエディタやEclipse，NetBeansなどのIDE，開発内容によって様々だと思います。
自分は簡単なコードだとScala(Metals)とScalaSnippetsの拡張機能を入れてVscodeを利用，Pluginを利用するものだったり複雑なコードだとIntelliJ IDEAを利用しています。
SparkアプリケーションをIntelliJ IDEAで開発する方法がとても便利だったので記録に残しておきます。
前提条件  Spark2.4がインストールされていること IntelliJIDEAがインストールされていること IntelliJIDEAでScalaを開発することができること
（N予備校のScala基礎コースで開発環境構築方法およびScalaの基礎が大変わかりやすかったです） Java8がインストールされていること JDK8を用いて開発できること
（[File]=&amp;gt;[Project Structure&amp;hellip;]=&amp;gt;左カラムのSDKsを選択=&amp;gt;JDKhomepathを変更からJDKの追加はできます）  環境  OS：Mac Spark：2.4.5 IntelliJIDEA：2019.3  プロジェクト作成 Scala=&amp;gt;sbt でプロジェクトを作成します。
Name，Location：任意
JDK：1.8
sbt，Scala：現在安定して動くもの（ここではsbt1.2.8，Scala2.11.0）
入力が終わったら[Finish]を押します。
Sparkを使えるようにする targetの配下にあるbuild.sbtにSparkのPluginを追加します。バージョンなどはMVNREPOSITORYサイトを参考にします。
name := &amp;#34;sparktest&amp;#34; version := &amp;#34;0.1&amp;#34; scalaVersion := &amp;#34;2.11.0&amp;#34; libraryDependencies ++= Seq( &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-core&amp;#34; % &amp;#34;2.4.0&amp;#34;, &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-sql&amp;#34; % &amp;#34;2.4.0&amp;#34;, &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-mllib&amp;#34; % &amp;#34;2.4.3&amp;#34;, &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-graphx&amp;#34; % &amp;#34;2.4.0&amp;#34;, &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-streaming&amp;#34; % &amp;#34;2.4.0&amp;#34; ) 無事インポートが成功したらSpark環境は完了です。</description>
    </item>
    
    <item>
      <title>pysparkをjupyter-notebookから使う</title>
      <link>https://srn221B.github.io/post/content1/</link>
      <pubDate>Sat, 28 Mar 2020 00:08:00 +0900</pubDate>
      
      <guid>https://srn221B.github.io/post/content1/</guid>
      <description>今までpysparkをいじるときはコンソールでネチネチやっていたのですが、 pythonをjupyter-notebookで開発しているときに便利だなあと思ったので、思い切ってpysparkをいじるときもjupyter-notebookを使うようにしました。その時の備忘録。
前提条件  Sparkがインストールされていること（この記事がわかりやすかったです。) jupyterがインストールされていること(pipまたはcondaでインストール)  環境  OS：Mac python：Python 3.7.6 Spark：2.4.5 jupyter-notebook：6.0.3  Sparkの設定 Spark上の環境ファイルのテンプレート(spark-env-sh.template)をコピーします。
$ cd $SPARK_HOME/conf $ cp spark-env.sh.template spark-env.sh コピーしたファイルを書き換えていきます。
$ vim spark-env.sh export PYSPARK_PYTHON=/usr/local/bin/python3 #pythonの場所 export PYSPARK_DRIVER_PYTHON=jupyter export PYSPARK_DRIVER_PYTHON_OPTS=&amp;#34;notebook&amp;#34; 環境変数の設定 以下の環境変数を.bashrc,.bash_profileに書きます。
$ vim ~/.bashrc SPARK_HOME=#Sparkをインストールした場所 PATH=$SPARK_HOME/bin:$PATH 動作確認 早速、jupyterを使って簡単な演算を実装して見ます。
$ pyspark http://localhost:8888/?token= *Token*が表示されます http://localhost:8888/loginを開きます。表示されたTokenを元にログインします。
Pythonファイルを作ります（New&amp;gt;python3）
以下のようにsc(SparkSession)が使えていたら完了。
終わり 快適な開発環境が整いました:heart_eyes:</description>
    </item>
    
    <item>
      <title>テスト２</title>
      <link>https://srn221B.github.io/post/test2/</link>
      <pubDate>Fri, 27 Mar 2020 20:16:08 +0900</pubDate>
      
      <guid>https://srn221B.github.io/post/test2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>テストページ</title>
      <link>https://srn221B.github.io/post/test/</link>
      <pubDate>Fri, 27 Mar 2020 20:01:14 +0900</pubDate>
      
      <guid>https://srn221B.github.io/post/test/</guid>
      <description>テストページです  2020/03/27 20:10 Friday  </description>
    </item>
    
  </channel>
</rss>