[{"categories":["技術"],"contents":"概要 学生時代のときに作ったものの知見や思い出を記憶が新しいうちに残しておく。 すぐ忘れてしまうので。\n作ったもの ほぼ時系列に載せていきます。ちなみに学生時代の私の創作意欲の原点は「何かを作りたくて作る」訳ではなく「使う技術について興味があるので作る」ことが多かったので、Web系だったりDB系だったり\u0026hellip;と全体的に一貫性がないです。あと、全体的に作って終わりで、改善をしていない。今から改善しようとしても環境がない。。。\n1. 2017年のアニメの円盤売上分析   なんで作ったの？\n データベーススペシャリスト試験を受験したときに出てきた「ディメンショナルモデリング（スタースキーマ）」が気になったから。    ディメンショナルモデリングとは？\n １つのファクトテーブル(売上や利益などの分析軸)と複数のディメンションテーブル(販売時期や産地などの分析軸をみる視点)で構成されるデータ分析に用いられるデータ構造の概念のようなものです。 カラム型ストレージが主流になる前は、繰り返されるデータをなくすことがストレージコストの安さにつながっていたのと、パフォーマンスが良かったことから、この構造が良しとされていたそうです。 現在はこのような構造は使わなく、データ分析に用いられる構造はワイドテーブルにまとめられているものが多いです。  参考：https://dev.classmethod.jp/articles/kimball-in-the-context-of-the-modern-data-warehouse-whats-worth-keeping-and-whats-not/      どのような手順で作ったの？\n データ構造をディメンショナルモデリングで考える。\n ネット上にある情報をもとに手作業でデータをCSVに入力-\u0026gt;MySQLに挿入。  アニメDVD・BD売り上げ一覧表まとめWikiとか、Wikipediaとか      わかったこと\n 日曜アニメの円盤売り上げが高い\n 特にこの年の日曜アニメは原作がゲームのものの売上が高かったようです。  調べてみたら「アイマス」とか「グランブル」とか   あと漫画は、作品数は多いが売上は高くない。\n   ゲームの中でも特にネトゲ・ソシャゲ\n 話数は売上に関係ない。\n Twitterの頑張りはあまり影響しない。\n    そのほかわかったこと\n データの手入力はつらい  正直、学生で時間があったからできたことだなと思います。   データをBackupしておけば良かった  今改めてデータを見ると「曜日毎の話数の平均は？」「売上が低いアニメに共通すること」などが気になります。。。使わなくなったデータでもどこかにBackupしておくことの大切さを実感しました。      2. TwitterAPIを使ったTwitterフォロー関係のNeo4jでの可視化  なんで作ったの？  Neo4jについて気になったから。   Neo4jとは？  グラフ理論をデータベースエンジンの設計思想として採用しているDBMSです。2000年頃、RDBMSのパフォーマンス改善に取り組んだのが開発のきっかけとなっているようです。グラフDBはNeo4j以外にもFlockDB,InfiniteGraph,AllgroGraphなどがあります。クエリ言語はSQLライクなCypherと呼ばれるJOINを得意とするクエリを使います。 例えば「任意の二人のフォロワーにフォローされている人を知りたい」というとき  通常のDBでは以下のようなDDLとDMLで抽出するかと思います。  /* DDL */ CREATE TABLE twitter_relationship( twitter_id string, follow_twitter_id string ) /* 挿入DML */ INSERT INTO twitter_relationship VALUES(\u0026#34;twitter_id1\u0026#34;,\u0026#34;twitter_id2\u0026#34;) /* 抽出DML 細かいところは異なるかも */ SELECT S.follow_twitter_id FROM twitter_relationship AS S INNER JOIN twitter_relationship AS T ON S.twitter_id = T.twitter_id AND S.follow_twitter_id = T.follow_twitter_id INNER JOIN twitter_relationship AS T2 ON T1.twitter_id = T2.twitter_id AND T1.follow_twitter_id = T2.follow_twitter_id WHERE T.twitter_id = \u0026lt;任意のフォロワー1\u0026gt; AND T2.twitter_id = \u0026lt;任意のフォロワー2\u0026gt;  Cypherだと以下のようなシンプルなDDLとDMLで抽出できます。  /* DDL 挿入DML */ MERGE (p1:People{twitter_id:\u0026#34;twitter_id1\u0026#34;}) MERGE (p2:People{twitter_id:\u0026#34;twitter_id2\u0026#34;}) CREATE UNIQUE (p1)-[:Follow]-\u0026gt;(p2) /* 抽出DML */ MATCH n=(person:People)-[:Follow]-\u0026gt;(:People)\u0026lt;-[:Follow]-(person2:People) WHERE person.twitter_id = \u0026lt;任意のフォロワー1\u0026gt; and person2.twitter_id = \u0026lt;任意のフォロワー2\u0026gt; return n  また上記結果はNeo4jのWebインタフェースを使うとわかりやすく可視化されます。\n 他にもいいこと多いみたいです     （前置き長くなりましたが）どのような手順で作ったの？  Neo4jを用意する  あまり覚えていないです。。DockerのImageを使った気がします（https://hub.docker.com/_/neo4j）   Pythonを組み立てる  https://github.com/srn221B/Show_relationships/blob/master/program.py     わかったこと  Neo4jについて  作っているときに参考にした技術書：https://www.amazon.co.jp/dp/B0171LWZ9O/ref=dp-kindle-redirect?_encoding=UTF8\u0026amp;btkr=1     今思うこと  GraphDBってどういうときに使うのだろうか  2年ぐらいしか仕事していない身で生意気なことを言いますが、業務でお客様先のデータパイプラインをいくつか作りましたが、GraphDBが必要だとなることは特になかった気がします。SQLを覚えることに苦労するデータサイエンティストもいる中、Cypherも扱えるデータサイエンティストはいるのだろうか\u0026hellip;と思うところがあります。 RDB使っていて辛いというところは使っているようです。      3. MySQLのDMLの自動化  なんで作ったの？  MySQLでの操作を楽にしたかったから。  データを見るためにCLIだとUSE データベース-\u0026gt;SELECT * FROM テーブル名するのは面倒\u0026hellip;     どうやって作ったの？  この時はあまりShellScriptのことをわかっていなかったので、その理解のためにもShellScriptを使用しました。  データの表示  https://github.com/srn221B/mysql_tool/blob/master/Show_Table_Data.sh   データの表示（抽出条件あり）  https://github.com/srn221B/mysql_tool/blob/master/Show_Data_Targ.sh   など     わかったこと  自動化は楽。  例えばデータの表示は以下のような操作で行えます。  SELECT * FROM database1.table1;  抽出条件ありのデータ表示は以下  SELECT * WHERE database1.table2 WHERE table_column2 = \u0026#39;400\u0026#39;;    今思うこと  仕事でMySQL使わない。。  使う環境が今後もない気がします。。      4. Djangoを用いたWebApplicationとPySparkを使ったレコメンド  なんで作ったの？  DjangoとSparkについて気になったから   Djangoとは？  Pythonで記述されたWebアプリケーションフレームワーク。Youtube,Pinterestなどこのフレームワークを使っているようです。   Sparkとは？  巨大なデータに対してメモリを使って分散処理を行うフレームワーク。処理が速いだけではなく、SparkStreamingを使用してリアルタイムでデータ処理をしたり、MLlibを使用して機械学習をしたりすることができます。   どうやって作ったの？  作りたいものの設計を考える。  実装 友人にデータ入力を手伝ってもらう。   できたもの  Filmarksのようなものを想像していただけるとわかるかなと思います。  任意の作品のジャケット写真をクリックすると-\u0026gt;作品詳細がポップアップで表示され、そこからレビュー投稿画面に遷移-\u0026gt;レビューを投稿するとユーザ・レビューデータがレビュー履歴TBLに保存される流れです。 定期的に上記TBL内にあるユーザ・レビューデータをもとに、Pysparkの協調フィルタリング(ALSアルゴリズム)を使って、どのユーザに何の作品がどのくらい(rating)推薦されるかを算出し、レコメンド作品管理TBLに保存します。\n   script:https://github.com/srn221B/Recommended_pyspark/blob/master/Program.py   わかったこと  機械学習するには全然データが足りない。  友人20名ほどにレビューをお願いしてもらいましたが、大体みんな同じような作品が推薦されてしまいました。   Django、Web開発しやすい  言語がPythonでなれているというのもありますが、特に便利だなあと思ったのが、データ操作ができる管理画面が自動で作られることです。  https://codor.co.jp/django/admin-and-data        やっておいて良かったこと  結果の見える努力  これは上記作ったものや、資格も含めです。「任意の分野の本をn本読みました」「任意の資格の勉強をしていました」などの結果の見えない努力に注力しすぎなくて良かったなと思います。    やってなくて後悔したこと  チーム開発  仕事でコードレビューのときに初歩的な指摘をされるのはもちろんですが、こちらがレビュー側になったときに「何もコメントすることがない」となることが多々ありました。こういうときにもっと学生のうちからチーム開発をしていれば、他の人のコードから知見を得られることができたし、レビューで貢献できることは何かを早いうちから身につけることができたのではと思います。  社会人になってから熟読した本  Python実践入門 テスト騒動Python リーダブルコード       アルゴリズム  足切りのためか、データエンジニアのポジションでもコーディング力を測るWebテストが選考にある会社はたくさんあります。。転職のために勉強するのが、個人的に面倒だったので学生のうちに基本的なところは完璧にしておくべきだったなあとおもいました。   英語  データの分野（特に分析基盤あたり）、日本語のドキュメントや知見がない上にドキュメントが更新されていないことが多々\u0026hellip;ソースコードレベルにデバックできるほどの英語力を早くから身につけておけばなあと思いました。    ","permalink":"https://467tn.com/blog/content6/","tags":["雑ログ"],"title":"学生時代の作ったものの記録"},{"categories":["技術"],"contents":"こんにちは ブログ書くの久々すぎて書き方忘れた。。。うそです。\n去年の卒研終わったあたりから作り始めたこのサイトですが、あまりにも初期状態すぎたので、重い腰をあげて「HugoのthemaをBeautifulHugoからCupperに変更」「repositoryを整理」の２点をしました。一新したのでブログ書くモチベもこれで上がるはず（？）仕事もだいぶ慣れてきたので2022年は些細なことでももう少し書いていきたいな。。。\nタイトル通り 「Dockerを使ったGrafanaのDatasourceとDashboardの起動時追加方法」のメモを簡単に備忘録としてまとめておきます。\nDatasourceの起動時追加 Dashboardの起動時追加を設定する前に、まずはこっちを作っていきます。\n構成 . ├── docker-compose.yml ├── flask │ ├── Dockerfile │ └── app │ └── app.py ├── grafana │ ├── Dockerfile │ └── datasource.yml └── prometheus.yml ここでは grafanaで可視化するサーバーはprometheusで、prometheusでの監視はflaskを用いた自作exporterとしています。\nflask/Dockerfile FROM ubuntu:latest RUN apt-get update RUN apt-get install python3 python3-pip -y RUN pip3 install flask prometheus-client RUN mkdir /app flask/app.py pythonのPrometheusClientライブラリを使ってexporter化しましした。testしやすいようにcurl http://localhost:3000/hogeでGauge型のmetricsが増減するexporterです。\nfrom flask import Flask,render_template,request import json import queue from werkzeug.middleware.dispatcher import DispatcherMiddleware from prometheus_client import make_wsgi_app,Gauge  app = Flask(__name__) G1 = Gauge(\u0026#39;Gauge1\u0026#39;,\u0026#39;Gauge test\u0026#39;) G2 = Gauge(\u0026#39;Gauge2\u0026#39;,\u0026#39;Gauge test\u0026#39;)  @app.route(\u0026#39;/upG1\u0026#39;,methods=[\u0026#34;GET\u0026#34;]) def upG1():  G1.inc()  return \u0026#34;upG1\u0026#34;  @app.route(\u0026#39;/upG2\u0026#39;,methods=[\u0026#34;GET\u0026#34;]) def upG2():  G2.inc()  return \u0026#34;upG2\u0026#34;  @app.route(\u0026#39;/downG1\u0026#39;,methods=[\u0026#34;GET\u0026#34;]) def downG1():  G1.dec()  return \u0026#34;downG1\u0026#34;  @app.route(\u0026#39;/downG2\u0026#39;,methods=[\u0026#34;GET\u0026#34;]) def downG2():  G2.dec()  return \u0026#34;downG2\u0026#34;  app.wsgi_app = DispatcherMiddleware(app.wsgi_app, {  \u0026#39;/metrics\u0026#39;: make_wsgi_app() })  if __name__ == \u0026#39;__main__\u0026#39;:  G1.set(0)  G2.set(0)  app.run(host=\u0026#39;0.0.0.0\u0026#39;,port=5000) grafana/Dockerfile とりあえず最低限。\nFROM grafana/grafana:master COPY ./datasource.yml /etc/grafana/provisioning/datasources/ grafana/datasource.yml 起動時追加するdatasourceについて書きます。詳しいパラメータについてはGrafanaの公式ドキュメントに書いてあります。\ndatasources:  - name: prometheus  type: prometheus  access: proxy  url: \u0026#34;http://prometheus:9090\u0026#34;  # 複数書きたい場合はこんな感じで  #- name: prometheus  # type: prometheus  # access: proxy  # url: \u0026#34;http://prometheus:9090\u0026#34; prometheus.yml 15秒間隔でexporterからmetricsを取得するように設定しています。\nglobal:  scrape_interval: 15s  evaluation_interval: 15s  scrape_configs:  - job_name: \u0026#39;prometheus\u0026#39;  static_configs:  - targets: [\u0026#39;flask:5000\u0026#39;] docker-compose.yml 上で書いていったものをまとめていきます。\nversion: \u0026#34;3\u0026#34; services:  flask:  build: ./flask  command: python3 app/app.py  volumes:  - ./flask/app:/app  ports:  - 5000:5000  prometheus:  image: prom/prometheus  volumes:  - ./prometheus.yml:/etc/prometheus/prometheus.yml  ports:  - 9090:9090  grafana:  build: ./grafana  ports:  - 3000:3000  environment:  - GF_SECURITY_ADMIN_PASSWORD=password  - GF_USERS_ALLOW_SIGN_UP=false 起動確認 docker-compose buildとdocker-compose up -dを行って３つのコンテナが立ち上がっているのを確認した後、http://localhost:3000でGrafanaへ接続します。ID：admin、PASSWORD：passwordでsign inし、Configuration\u0026gt;Data SourcesにPrometheusがあれば完了です。\nDashboardの起動時追加方法について 上記DataSourceの起動時追加設定をした後に、進めていきます。\n構成 ├── docker-compose.yml ├── flask │ ├── Dockerfile │ └── app │ └── app.py ├── grafana │ ├── Dockerfile　#変更します │ ├── dashboard.yml　\u0008#作成します │ ├── datasource.yml　│ └── prometheus #作成します │ └── \u0026lt;hoge\u0026gt;.json　#作成します └── prometheus.yml dashboardの起動時追加設定ファイルdashboard.yml、dashboardの基盤ファイル\u0026lt;hoge\u0026gt;.jsonを作成し、それらファイルをコンテナへ置くためにgrafanaのDockerfileを変更します。\ngrafana/prometheus/.json http://localhost:3000でGrafanaへ接続し、 Create\u0026gt;Dashboardで起動時に追加したいDashboardの基盤ファイルを作っていきます。\nShare dashboard\u0026gt;Export\u0026gt;Save to fileでJSONファイルを出力し、./grafana/prometheus配下にファイルをおきます。 grafana/dashboard.yml 起動時追加するdashboardについて書きます。こちらも、詳しいパラメータについてはGrafanaの公式ドキュメントに書いてあります。\napiVersion: 1 providers:  - name: \u0026#39;prometheus metrics\u0026#39;  orgId: 1  folder: \u0026#39;\u0026#39;  folderUid: \u0026#39;\u0026#39;  type: file  disableDeletion: false  updateIntervalSeconds: 10  allowUiUpdates: false  options:  path: /etc/grafana/provisioning/dashboards/prometheus  foldersFromFileStructure: true grafana/Dockerfile 作成したファイルをコンテナに置くために以下に変更。\nFROM grafana/grafana:master COPY ./datasource.yml /etc/grafana/provisioning/datasources/ COPY ./dashboard.yml /etc/grafana/provisioning/dashboards/  COPY ./prometheus /etc/grafana/provisioning/dashboards/prometheus 起動確認 DataSourceの設定した時と同じく、docker-compose buildとdocker-compose up -dを行って３つのコンテナが立ち上がっているのを確認した後、http://localhost:3000でGrafanaへ接続します。ID：admin、PASSWORD：passwordでsign inし、dashboardsが自動作成されていれば完了です。\n","permalink":"https://467tn.com/blog/content4/","tags":["Docker","Grafana","Prometheus","Flask"],"title":"Dockerを使ったGrafanaのDatasourceとDashboardの起動時追加方法"},{"categories":["技術",""],"contents":"タイトル通り。zeppelinからcassandraとsparkを使うまで簡単に構築メモ。 kubernetesとhelm 3.3.1を使って構築する。sparkとzeppelinのインストール→Cassandraのインストールの流れで説明。\nsparkとzeppelin  リポジトリはこれ デプロイすると、Podが動く。 Zepplin上のファイルはデフォルトで記憶される。  values.yamlにおいてServicePortの編集  デフォルトのままだとsparkWebUIとzeppelinのServicePortが競合する。なので、どちらかのServicePortを変更する。 ここではsparkWebUIのServicePortを8080から8081に変更する。  $ vim spark/values.yaml ... WebUi:  Name: webui  ServicePort: 8081  ContainerPort: 8080 ... インストール $ helm install spark ./spark インストール確認  defaultのnamespaceにインストールされていることを確認。  $ kubectl get pod spark-master-877d79587-shv7g 1/1 Running spark-worker-87d4579f4-7fsg4 1/1 Running spark-worker-87d4579f4-lb7ft 1/1 Running spark-worker-87d4579f4-tvcnb 1/1 Running spark-zeppelin-85d6c884d8-289mb 1/1 Running zeppelinとSparkWebUIにアクセスの確認  ブラウザ上からlocalhost:8080とlocalhost:8081へのアクセスへの確認ができれば完了。\n  cassandra  リポジトリはこれ user/passwordを設定しやすそうだったのでbitnamiを選択。  values.yamlにおいてuser/passwordの編集  デフォルトのままだとuserはcassandra、passwordはランダムで作られる。なので、指定したもので作られるように設定する。 ここではuserをcassandra、passwordをcassandraとする。  ... dbUser:  user: cassandra  forcePassword: false  password: cassandra ... インストール $ helm install cassandra ./cassandra インストール確認  defaultのnamespaceにインストールされていることを確認。  $ kubectl get pod -n default cassandra-0 1/1 Running Zeppelin上のCassandraのInterpreter  zeppelinからcassandraへ接続する為の設定を変更。  cassandraが動いているClusterIPを確認 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cassandra ClusterIP 10.99.62.208 \u0026lt;none\u0026gt; 9042/TCP,9160/TCP,8080/TCP 1d Interpreterの設定変更  ブラウザ上からhttp://localhost:8080/#/interpreterを開く。 上記で確認した情報を元にcassandra.credentials.password、cassandra.credentials.username、cassandra.host、cassandra.native.portの設定を変更。\n  完了確認 ","permalink":"https://467tn.com/blog/content3/","tags":["spark","zeppelin","kubernetes","cassandra"],"title":"zeppelinからcassandraとsparkを使う"},{"categories":["技術"],"contents":"前回に引き続きSpark環境構築系のおはなし。\nScala、何で開発してますか\nVScode，AtomなどのエディタやEclipse，NetBeansなどのIDE，開発内容によって様々だと思います。\n自分は簡単なコードだとScala(Metals)とScalaSnippetsの拡張機能を入れてVscodeを利用，Pluginを利用するものだったり複雑なコードだとIntelliJ IDEAを利用しています。\nSparkアプリケーションをIntelliJ IDEAで開発する方法がとても便利だったので記録に残しておきます。\n前提条件  Spark2.4がインストールされていること IntelliJIDEAがインストールされていること IntelliJIDEAでScalaを開発することができること Java8がインストールされていること JDK8を用いて開発できること\n（[File]=\u0026gt;[Project Structure\u0026hellip;]=\u0026gt;左カラムのSDKsを選択=\u0026gt;JDKhomepathを変更からJDKの追加はできます）  環境  OS：Mac Spark：2.4.5 IntelliJIDEA：2019.3  プロジェクト作成 Scala=\u0026gt;sbt でプロジェクトを作成します。\nName，Location：任意\nJDK：1.8\nsbt，Scala：現在安定して動くもの（ここではsbt1.2.8，Scala2.11.0）\n入力が終わったら[Finish]を押します。\nSparkを使えるようにする targetの配下にあるbuild.sbtにSparkのPluginを追加します。バージョンなどはMVNREPOSITORYサイトを参考にします。\nname := \u0026#34;sparktest\u0026#34;  version := \u0026#34;0.1\u0026#34;  scalaVersion := \u0026#34;2.11.0\u0026#34;  libraryDependencies ++= Seq(  \u0026#34;org.apache.spark\u0026#34; %% \u0026#34;spark-core\u0026#34; % \u0026#34;2.4.0\u0026#34;,  \u0026#34;org.apache.spark\u0026#34; %% \u0026#34;spark-sql\u0026#34; % \u0026#34;2.4.0\u0026#34;,  \u0026#34;org.apache.spark\u0026#34; %% \u0026#34;spark-mllib\u0026#34; % \u0026#34;2.4.3\u0026#34;,  \u0026#34;org.apache.spark\u0026#34; %% \u0026#34;spark-graphx\u0026#34; % \u0026#34;2.4.0\u0026#34;,  \u0026#34;org.apache.spark\u0026#34; %% \u0026#34;spark-streaming\u0026#34; % \u0026#34;2.4.0\u0026#34; ) 無事インポートが成功したらSpark環境は完了です。\nコーディング src=\u0026gt;main=\u0026gt;scalaを右クリック　New=\u0026gt;Scala Classで任意の名前[Class]を作成します。\nMain.scalaを編集します。ここではJSONファイルを単語集計したものをテキストファイルに保存するコードを書いていきます。\nimport org.apache.spark.{SparkConf, SparkContext} import com.fasterxml.jackson.databind.ObjectMapper import com.fasterxml.jackson.module.scala.DefaultScalaModule  object example {  def main(args: Array[String]): Unit = {  // Sessionの生成  val conf = new SparkConf().setAppName(\u0026#34;appname\u0026#34;).setMaster(\u0026#34;local[*]\u0026#34;)  val sc = new SparkContext(conf)   val jsonLines = sc.parallelize(Array(  \u0026#34;\u0026#34;\u0026#34;{\u0026#34;name\u0026#34; : \u0026#34;Apple\u0026#34;,\u0026#34;num\u0026#34;: 1}\u0026#34;\u0026#34;\u0026#34;,  \u0026#34;\u0026#34;\u0026#34;{\u0026#34;name\u0026#34; : \u0026#34;Orange\u0026#34;,\u0026#34;num\u0026#34;: 4}\u0026#34;\u0026#34;\u0026#34;,  \u0026#34;\u0026#34;\u0026#34;{\u0026#34;name\u0026#34; : \u0026#34;Apple\u0026#34;,\u0026#34;num\u0026#34;: 2}\u0026#34;\u0026#34;\u0026#34;,  \u0026#34;\u0026#34;\u0026#34;{\u0026#34;name\u0026#34; : \u0026#34;Peach\u0026#34;,\u0026#34;num\u0026#34;: 1}\u0026#34;\u0026#34;\u0026#34;  ))   val result = jsonLines.mapPartitions {lines =\u0026gt;  //JSONのパーサを初期化  val mapper = new ObjectMapper()  mapper.registerModule(DefaultScalaModule)   //JSON文字列をパースし、\u0026#34;name\u0026#34;と\u0026#34;num\u0026#34;のペアで返却  lines.map {line =\u0026gt;  val f = mapper.readValue(line,classOf[Map[String,String]])  (f(f\u0026#34;name\u0026#34;),f(\u0026#34;num\u0026#34;))  }   }  // savedirに集計結果を保存  result.saveAsTextFile(\u0026#34;savedir\u0026#34;)  } } 実行 Terminal上でjarファイルを生成します。\n$ sbt package spark-submitを使って実行\n$ $SPARK_HOME/bin/spark-submit savedir上に集計結果が保存されていることを確認します。\n$ cat savedir_part* (Apple,1) (Orange,4) (Apple,2) (Peach,1) 終わり 実行毎にspark-submit〜と打つのは煩雑ですが、予測候補が出てくるのは開発する上での嬉しい＆助かる機能です🥰\n","permalink":"https://467tn.com/blog/content2/","tags":["spark","IntelliJIDEA"],"title":"IntelliJ IDEAでのSparkアプリケーション開発方法（Scala編）"},{"categories":["技術"],"contents":"今までpysparkをいじるときはコンソールでネチネチやっていたのですが、 pythonをjupyter-notebookで開発しているときに便利だなあと思ったので、思い切ってpysparkをいじるときもjupyter-notebookを使うようにしました。その時の備忘録。\n前提条件  Sparkがインストールされていること（この記事がわかりやすかったです。) jupyterがインストールされていること(pipまたはcondaでインストール)  環境  OS：Mac python：Python 3.7.6 Spark：2.4.5 jupyter-notebook：6.0.3  Sparkの設定 Spark上の環境ファイルのテンプレート(spark-env-sh.template)をコピーします。\n$ cd $SPARK_HOME/conf $ cp spark-env.sh.template spark-env.sh コピーしたファイルを書き換えていきます。\n$ vim spark-env.sh $ export PYSPARK_PYTHON=/usr/local/bin/python3 #pythonの場所 $ export PYSPARK_DRIVER_PYTHON=jupyter $ export PYSPARK_DRIVER_PYTHON_OPTS=\u0026#34;notebook\u0026#34; 環境変数の設定 以下の環境変数を.bashrc,.bash_profileに書きます。\nvim ~/.bashrc ... SPARK_HOME=#Sparkをインストールした場所 PATH=$SPARK_HOME/bin:$PATH ... 動作確認 早速、jupyterを使って簡単な演算を実装して見ます。\n$ pyspark http://localhost:8888/?token= *Token*が表示されます http://localhost:8888/loginを開きます。表示されたTokenを元にログインします。\nPythonファイルを作ります（New\u0026gt;python3）\n以下のようにsc(SparkSession)が使えていたら完了。\n終わり 快適な開発環境が整いました:heart_eyes:\n","permalink":"https://467tn.com/blog/content1/","tags":["pyspark","jupyter"],"title":"pysparkをjupyter-notebookから使う"}]