<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>467のメモ帳</title>
    <link>https://srn221B.github.io/</link>
    <description>Recent content on 467のメモ帳</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 20 Sep 2020 20:51:54 +0900</lastBuildDate>
    
	<atom:link href="https://srn221B.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>zeppelinからcassandraとsparkを使う</title>
      <link>https://srn221B.github.io/post/content3/</link>
      <pubDate>Sun, 20 Sep 2020 20:51:54 +0900</pubDate>
      
      <guid>https://srn221B.github.io/post/content3/</guid>
      <description>タイトル通り。zeppelinからcassandraとsparkを使うまで簡単に構築メモ。 kubernetesとhelm 3.3.1を使って構築する。sparkとzeppelinのインストール→Cassandraのインストールの流れで説明。
Pod単位で弄れるが為だけにkubernetesを使います。Namespace分けたりIngress追加したりなどIaCならではの機能を活かした実装はここではしません。悪しからず。
sparkとzeppelin  リポジトリはこれ デプロイすると、MasterPod1台、WorkerPod1台が動く。 Zepplin上のファイルはデフォルトで記憶される。  values.yamlにおいてServicePortの編集  デフォルトのままだとsparkWebUIとzeppelinのServicePortが競合する。なので、どちらかのServicePortを変更する。 ここではsparkWebUIのServicePortを8080から8081に変更する。  $ vim spark/values.yaml ... WebUi: Name: webui ServicePort: 8081 ContainerPort: 8080 ... インストール $ helm install spark ./spark インストール確認  defaultのnamespaceにインストールされていることを確認。  $ kubectl get pod spark-master-877d79587-shv7g 1/1 Running spark-worker-87d4579f4-7fsg4 1/1 Running spark-worker-87d4579f4-lb7ft 1/1 Running spark-worker-87d4579f4-tvcnb 1/1 Running spark-zeppelin-85d6c884d8-289mb 1/1 Running zeppelinとSparkWebUIにアクセスの確認  ブラウザ上からlocalhost:8080とlocalhost:8081へのアクセスへの確認ができれば完了。
  cassandra  リポジトリはこれ user/passwordを設定しやすそうだったのでbitnamiを選択。  values.yamlにおいてuser/passwordの編集  デフォルトのままだとuserはcassandra、passwordはランダムで作られる。なので、指定したもので作られるように設定する。 ここではuserをcassandra、passwordをcassandraとする。  .</description>
    </item>
    
    <item>
      <title>IntelliJ IDEAでのSparkアプリケーション開発方法（Scala編）</title>
      <link>https://srn221B.github.io/post/content2/</link>
      <pubDate>Sat, 04 Apr 2020 15:40:59 +0900</pubDate>
      
      <guid>https://srn221B.github.io/post/content2/</guid>
      <description>前回に引き続きSpark環境構築系のおはなし。
Scala、何で開発してますか
VScode，AtomなどのエディタやEclipse，NetBeansなどのIDE，開発内容によって様々だと思います。
自分は簡単なコードだとScala(Metals)とScalaSnippetsの拡張機能を入れてVscodeを利用，Pluginを利用するものだったり複雑なコードだとIntelliJ IDEAを利用しています。
SparkアプリケーションをIntelliJ IDEAで開発する方法がとても便利だったので記録に残しておきます。
前提条件  Spark2.4がインストールされていること IntelliJIDEAがインストールされていること IntelliJIDEAでScalaを開発することができること
（N予備校のScala基礎コースで開発環境構築方法およびScalaの基礎が大変わかりやすかったです） Java8がインストールされていること JDK8を用いて開発できること
（[File]=&amp;gt;[Project Structure&amp;hellip;]=&amp;gt;左カラムのSDKsを選択=&amp;gt;JDKhomepathを変更からJDKの追加はできます）  環境  OS：Mac Spark：2.4.5 IntelliJIDEA：2019.3  プロジェクト作成 Scala=&amp;gt;sbt でプロジェクトを作成します。
Name，Location：任意
JDK：1.8
sbt，Scala：現在安定して動くもの（ここではsbt1.2.8，Scala2.11.0）
入力が終わったら[Finish]を押します。
Sparkを使えるようにする targetの配下にあるbuild.sbtにSparkのPluginを追加します。バージョンなどはMVNREPOSITORYサイトを参考にします。
name := &amp;#34;sparktest&amp;#34; version := &amp;#34;0.1&amp;#34; scalaVersion := &amp;#34;2.11.0&amp;#34; libraryDependencies ++= Seq( &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-core&amp;#34; % &amp;#34;2.4.0&amp;#34;, &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-sql&amp;#34; % &amp;#34;2.4.0&amp;#34;, &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-mllib&amp;#34; % &amp;#34;2.4.3&amp;#34;, &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-graphx&amp;#34; % &amp;#34;2.4.0&amp;#34;, &amp;#34;org.apache.spark&amp;#34; %% &amp;#34;spark-streaming&amp;#34; % &amp;#34;2.4.0&amp;#34; ) 無事インポートが成功したらSpark環境は完了です。</description>
    </item>
    
    <item>
      <title>pysparkをjupyter-notebookから使う</title>
      <link>https://srn221B.github.io/post/content1/</link>
      <pubDate>Sat, 28 Mar 2020 00:08:00 +0900</pubDate>
      
      <guid>https://srn221B.github.io/post/content1/</guid>
      <description>今までpysparkをいじるときはコンソールでネチネチやっていたのですが、 pythonをjupyter-notebookで開発しているときに便利だなあと思ったので、思い切ってpysparkをいじるときもjupyter-notebookを使うようにしました。その時の備忘録。
前提条件  Sparkがインストールされていること（この記事がわかりやすかったです。) jupyterがインストールされていること(pipまたはcondaでインストール)  環境  OS：Mac python：Python 3.7.6 Spark：2.4.5 jupyter-notebook：6.0.3  Sparkの設定 Spark上の環境ファイルのテンプレート(spark-env-sh.template)をコピーします。
$ cd $SPARK_HOME/conf $ cp spark-env.sh.template spark-env.sh コピーしたファイルを書き換えていきます。
$ vim spark-env.sh export PYSPARK_PYTHON=/usr/local/bin/python3 #pythonの場所 export PYSPARK_DRIVER_PYTHON=jupyter export PYSPARK_DRIVER_PYTHON_OPTS=&amp;#34;notebook&amp;#34; 環境変数の設定 以下の環境変数を.bashrc,.bash_profileに書きます。
$ vim ~/.bashrc SPARK_HOME=#Sparkをインストールした場所 PATH=$SPARK_HOME/bin:$PATH 動作確認 早速、jupyterを使って簡単な演算を実装して見ます。
$ pyspark http://localhost:8888/?token= *Token*が表示されます http://localhost:8888/loginを開きます。表示されたTokenを元にログインします。
Pythonファイルを作ります（New&amp;gt;python3）
以下のようにsc(SparkSession)が使えていたら完了。
終わり 快適な開発環境が整いました:heart_eyes:</description>
    </item>
    
    <item>
      <title>About me</title>
      <link>https://srn221B.github.io/page/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srn221B.github.io/page/about/</guid>
      <description>Who am i  じこしょうかい
 watashi  イギリス産のうさぎ（1998-早生まれ） 静岡の東部育ちで2020年春から上京 最弱データエンジニア（ビッグデータ関連） ミドルとインフラあたりをさまよってます。  shikaku/sanka  Security Camp 全国 2018 応用情報技術者試験(2016秋) 情報処理安全確保支援士試験(2017春) データベーススペシャリスト試験(2018春) ネットワークスペシャリスト試験(2018夏) CBss 2018/2019  suki  イギリス（York,Cotsworlds) 進撃の巨人 python アヒージョ  links  filmarks Animetick 旧ブログ  about アヒージョ  牡蠣のアヒージョ 海老のアヒージョ チーズのアヒージョ ウインナーのアヒージョ ブロッコリーのアヒージョ マッシュルームのアヒージョ ミニトマトのアヒージョ アボカドのアヒージョ パプリカのアヒージョ じゃがいものアヒージョ  </description>
    </item>
    
  </channel>
</rss>